# Environment Configuration Template
# Copy this file to .env and update with your actual values

# LLM / Ollama Configuration
# Set provider to 'ollama' to use a local Ollama server for embeddings and LLM
# Supported values: 'openai' (default) or 'ollama'
LLM_PROVIDER=openai
EMBEDDINGS_PROVIDER= # Optional, leave blank to use same as LLM_PROVIDER

# OpenAI API Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4
OPENAI_PROXY=  # Optional: Set if you need to use a proxy

GPT5_REASONING_EFFORT=minimal # Optional for GPT5 only: 'minimal', 'low', 'medium', 'high'
GPT5_TEXT_VERBOSITY=medium # Optional for GPT5 only: 'low', 'medium', 'high'

# Neo4j Configuration
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=your_neo4j_password_here

# If using Ollama, set the base URL (e.g. http://host.docker.internal:11434 or http://ollama:11434)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
OLLAMA_API_KEY=   # Not required for local Ollama server

# Embedding Configuration
EMBEDDING_MODEL=text-embedding-ada-002
EMBEDDING_CONCURRENCY=3

# Reranking Configuration
RERANKER_MODEL=bge-reranker-v2-m3
RERANKER_TOP_K=  # Optional - number of chunks returned by reranker - return all if not set

# Document Processing Configuration
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# Application Configuration
LOG_LEVEL=INFO
MAX_UPLOAD_SIZE=104857600

# Feature Flags
ENABLE_RERANKING=true
ENABLE_ENTITY_EXTRACTION=true
ENABLE_QUALITY_SCORING=true
ENABLE_DELETE_OPERATIONS=true